{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CS5228 project\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# visualiation\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier # decision Tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.feature_selection import RFE # for feature selection of LR\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into dataframe\n",
    "f_data = pd.read_csv('financial_data.csv', na_values=['?']) \n",
    "revealed = pd.read_csv('revealed_businesses.csv')\n",
    "t_data = pd.read_csv('testing_data.csv', na_values=['?'])\n",
    "\n",
    "#display(f_data.head())\n",
    "#display(t_data.head())\n",
    "#display(revealed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean data\n",
    "#fdata = fdata.replace(0, np.nan) # assume 0 values also means missing\n",
    "f_data = f_data.fillna(f_data.mean())\n",
    "#tdata = tdata.replace(0, np.nan)\n",
    "t_data = t_data.fillna(t_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "fdatan = f_data.copy(deep=True)\n",
    "normald = StandardScaler()\n",
    "fdatan_1 = normald.fit_transform(fdatan.ix[:,fdatan.columns!=\"Var1\"]) # normalize all variables except Var1\n",
    "fdatan = pd.DataFrame(np.column_stack((fdatan[\"Var1\"].values,fdatan_1)),columns = fdatan.columns).set_index(fdatan.index)\n",
    "tdatan = t_data.copy(deep=True)\n",
    "normald = StandardScaler()\n",
    "tdatan_1 = normald.fit_transform(tdatan.ix[:,tdatan.columns!=\"Var1\"]) # normalize all variables except Var1\n",
    "tdatan = pd.DataFrame(np.column_stack((tdatan[\"Var1\"].values,tdatan_1)),columns = tdatan.columns).set_index(tdatan.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframe\n",
    "fdatan_merged = fdatan.merge(revealed, how=\"outer\")\n",
    "fdatan_train1 = fdatan.merge(revealed) # dataframe, whose bankruptcy status is known (0 = good standing, 1 = bankrupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = fdatan_train1.drop(columns=['Var1', 'Var66']) # Var 1 is company ID, Var 66 is the status\n",
    "y1 = fdatan_train1['Var66']\n",
    "t1 = tdatan.drop(columns=['Var1'])\n",
    "cID = t_data['Var1'].tolist() # use original values, since nothing is done in cID of tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset will be split on index: 3903'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3903, 64)\n",
      "(3903,)\n",
      "(976, 64)\n",
      "(976,)\n"
     ]
    }
   ],
   "source": [
    "#Set Splitting\n",
    "index_to_round = round(len(fdatan_train1.index)*0.8)\n",
    "display(\"Dataset will be split on index: {}\".format(index_to_round))\n",
    "\n",
    "x_training = x1.iloc[:index_to_round, :]\n",
    "y_training = y1.iloc[:index_to_round]\n",
    "\n",
    "\n",
    "x_testing = x1.iloc[index_to_round:, :]\n",
    "y_testing = y1.iloc[index_to_round:]\n",
    "\n",
    "columns_no_index = list(x_training)\n",
    "\n",
    "print(x_training.shape)\n",
    "print(y_training.shape)\n",
    "print(x_testing.shape)\n",
    "print(y_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9869331283627979 Test Score:0.9743852459016393\n",
      "Test F1:0.4897959183673469\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=0.5,\n",
    "                                 criterion='friedman_mse', min_samples_split=90, min_samples_leaf=1,\n",
    "                                 min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                 min_impurity_split=None, init=None, random_state=None, max_features='auto', verbose=0,\n",
    "                                 max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1,\n",
    "                                 n_iter_no_change=None, tol=0.0001)\n",
    "clf.fit(x_training, y_training)\n",
    "train_score = clf.score(x_training, y_training)\n",
    "test_score = clf.score(x_testing, y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0 Test Score:0.9825819672131147\n",
      "Test F1:0.6046511627906976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[946,  17],\n",
       "       [  0,  13]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=8, learning_rate=0.4, n_estimators=100, objective='binary:logistic',\n",
    "                    booster='gbtree', n_jobs=-1, gamma=0, min_child_weight=1,\n",
    "                    max_delta_step=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=0.9, base_score=0.5, random_state=0,\n",
    "                    missing=None)\n",
    "clf.fit(x_training[XGBFeatures], y_training)\n",
    "train_score = clf.score(x_training[XGBFeatures], y_training)\n",
    "test_score = clf.score(x_testing[XGBFeatures], y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing[XGBFeatures]))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))\n",
    "confusion_matrix(clf.predict(x_testing[XGBFeatures]),y_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection:\n",
    "\n",
    "Best 45 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_jobs=-1)\n",
    "#clf.fit(x_training, y_training)\n",
    "\n",
    "rfe = RFE(clf, 45)\n",
    "rfe = rfe.fit(x_training, y_training)\n",
    "# summarize the selection of the attributes\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = columns_no_index.copy()\n",
    "XGBFeatures = []\n",
    "for i, feature in enumerate(useful_features):\n",
    "    if rfe.support_[i]:\n",
    "        XGBFeatures.append(feature)\n",
    "#XGBFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "x_new = model.transform(x_training)\n",
    "new_params = model.get_support(True)\n",
    "#XGBFeatures = useful_features[new_params]\n",
    "#new_params\n",
    "XGBFeatures = np.asarray(useful_features)[new_params]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE Re-Sampling:\n",
    "sm = SMOTE(random_state=12)\n",
    "x_train_sm, y_train_sm = sm.fit_sample(x_training[XGBFeatures], y_training)\n",
    "x_train_sm = pd.DataFrame(x_train_sm, columns=XGBFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9976082912569758 Test Score:0.9651639344262295\n",
      "Test F1:0.46875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[927,  15],\n",
       "       [ 19,  15]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=3, learning_rate=0.25, n_estimators=125, verbosity=1, objective='binary:logistic',\n",
    "                    booster='gbtree', n_jobs=-1, gamma=0, min_child_weight=0,\n",
    "                    max_delta_step=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0,\n",
    "                    missing=None)\n",
    "clf.fit(x_train_sm, y_train_sm)\n",
    "train_score = clf.score(x_train_sm, y_train_sm)\n",
    "test_score = clf.score(x_testing[XGBFeatures], y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing[XGBFeatures]))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))\n",
    "confusion_matrix(clf.predict(x_testing[XGBFeatures]),y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 4709, 1: 170})\n",
      "Resampled dataset shape Counter({0: 170, 1: 170})\n"
     ]
    }
   ],
   "source": [
    "print('Original dataset shape %s' % Counter(y1))\n",
    "cc = ClusterCentroids(random_state=42)\n",
    "x_res, y_res = cc.fit_resample(x1[XGBFeatures], y1)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Means Adjusted Random Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "#Clustering:\n",
    "#KMeans:\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto',\n",
    "                verbose=0, random_state=0, copy_x=True, n_jobs=-1,\n",
    "                algorithm='auto').fit(x_res)\n",
    "ars_kmeans = adjusted_rand_score(kmeans.labels_, y_res)\n",
    "print('K-Means Adjusted Random Score: {}'.format(ars_kmeans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN Adjusted Rand Score: 0.013951941289629469\n",
      "Estimated number of clusters: 5\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN:\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=4, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30,\n",
    "                p=None, n_jobs=-1).fit(x1[XGBFeatures])\n",
    "ars_dbscan = adjusted_rand_score(dbscan.labels_, y1)\n",
    "print('DBSCAN Adjusted Rand Score: {}'.format(ars_dbscan))\n",
    "labels = dbscan.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIRCH Adjusted Rand Score: -0.00039304871653265765\n"
     ]
    }
   ],
   "source": [
    "#BIRCH:\n",
    "brc = Birch(threshold=0.5, branching_factor=50, n_clusters=2, compute_labels=True, copy=True)\n",
    "brc.fit(x1[XGBFeatures]) \n",
    "ars_birch = adjusted_rand_score(brc.labels_, y1)\n",
    "print('BIRCH Adjusted Rand Score: {}'.format(ars_birch))\n",
    "#brc.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
