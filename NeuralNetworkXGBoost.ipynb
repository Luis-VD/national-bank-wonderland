{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CS5228 project\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# visualiation\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier # decision Tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.feature_selection import RFE # for feature selection of LR\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "#Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load into dataframe\n",
    "f_data = pd.read_csv('financial_data.csv', na_values=['?']) \n",
    "revealed = pd.read_csv('revealed_businesses.csv')\n",
    "t_data = pd.read_csv('testing_data.csv', na_values=['?'])\n",
    "\n",
    "#display(f_data.head())\n",
    "#display(t_data.head())\n",
    "#display(revealed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean data\n",
    "#fdata = fdata.replace(0, np.nan) # assume 0 values also means missing\n",
    "f_data = f_data.fillna(f_data.mean())\n",
    "#tdata = tdata.replace(0, np.nan)\n",
    "t_data = t_data.fillna(t_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Normalizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data\n",
    "fdatan = f_data.copy(deep=True)\n",
    "normald = StandardScaler()\n",
    "fdatan_1 = normald.fit_transform(fdatan.ix[:,fdatan.columns!=\"Var1\"]) # normalize all variables except Var1\n",
    "fdatan = pd.DataFrame(np.column_stack((fdatan[\"Var1\"].values,fdatan_1)),columns = fdatan.columns).set_index(fdatan.index)\n",
    "tdatan = t_data.copy(deep=True)\n",
    "normald = StandardScaler()\n",
    "tdatan_1 = normald.fit_transform(tdatan.ix[:,tdatan.columns!=\"Var1\"]) # normalize all variables except Var1\n",
    "tdatan = pd.DataFrame(np.column_stack((tdatan[\"Var1\"].values,tdatan_1)),columns = tdatan.columns).set_index(tdatan.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dataframe\n",
    "fdatan_merged = fdatan.merge(revealed, how=\"outer\")\n",
    "fdatan_train1 = fdatan.merge(revealed) # dataframe, whose bankruptcy status is known (0 = good standing, 1 = bankrupt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = fdatan_train1.drop(columns=['Var1', 'Var66']) # Var 1 is company ID, Var 66 is the status\n",
    "y1 = fdatan_train1['Var66']\n",
    "t1 = tdatan.drop(columns=['Var1'])\n",
    "cID = t_data['Var1'].tolist() # use original values, since nothing is done in cID of tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset will be split on index: 3903'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3903, 64)\n",
      "(3903,)\n",
      "(976, 64)\n",
      "(976,)\n"
     ]
    }
   ],
   "source": [
    "#Set Splitting\n",
    "index_to_round = round(len(fdatan_train1.index)*0.8)\n",
    "display(\"Dataset will be split on index: {}\".format(index_to_round))\n",
    "\n",
    "x_training = x1.iloc[:index_to_round, :]\n",
    "y_training = y1.iloc[:index_to_round]\n",
    "\n",
    "\n",
    "x_testing = x1.iloc[index_to_round:, :]\n",
    "y_testing = y1.iloc[index_to_round:]\n",
    "\n",
    "columns_no_index = list(x_training)\n",
    "\n",
    "print(x_training.shape)\n",
    "print(y_training.shape)\n",
    "print(x_testing.shape)\n",
    "print(y_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9861644888547272 Test Score:0.9723360655737705\n",
      "Test F1:0.509090909090909\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=0.5,\n",
    "                                 criterion='friedman_mse', min_samples_split=90, min_samples_leaf=1,\n",
    "                                 min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0,\n",
    "                                 min_impurity_split=None, init=None, random_state=None, max_features='auto', verbose=0,\n",
    "                                 max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1,\n",
    "                                 n_iter_no_change=None, tol=0.0001)\n",
    "clf.fit(x_training, y_training)\n",
    "train_score = clf.score(x_training, y_training)\n",
    "test_score = clf.score(x_testing, y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.988982833717653 Test Score:0.9825819672131147\n",
      "Test F1:0.6046511627906976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[946,  17],\n",
       "       [  0,  13]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, objective='binary:logistic',\n",
    "                    booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1,\n",
    "                    max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n",
    "                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None,\n",
    "                    missing=None)\n",
    "clf.fit(x_training, y_training)\n",
    "train_score = clf.score(x_training, y_training)\n",
    "test_score = clf.score(x_testing, y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))\n",
    "confusion_matrix(clf.predict(x_testing),y_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection:\n",
    "\n",
    "Best 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_jobs=-1)\n",
    "#clf.fit(x_training, y_training)\n",
    "\n",
    "rfe = RFE(clf, 45)\n",
    "rfe = rfe.fit(x_training, y_training)\n",
    "# summarize the selection of the attributes\n",
    "#print(rfe.support_)\n",
    "#print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_features = columns_no_index.copy()\n",
    "XGBFeatures = []\n",
    "for i, feature in enumerate(useful_features):\n",
    "    if rfe.support_[i]:\n",
    "        XGBFeatures.append(feature)\n",
    "#XGBFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0 Test Score:0.985655737704918\n",
      "Test F1:0.7407407407407408\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=8, learning_rate=0.35, n_estimators=100, verbosity=1, objective='binary:logistic',\n",
    "                    booster='gbtree', n_jobs=-1, gamma=0, min_child_weight=1,\n",
    "                    max_delta_step=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=600, base_score=0.5, random_state=0,\n",
    "                    missing=None)\n",
    "clf.fit(x_training[XGBFeatures], y_training)\n",
    "train_score = clf.score(x_training[XGBFeatures], y_training)\n",
    "test_score = clf.score(x_testing[XGBFeatures], y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing[XGBFeatures]))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to predictions.csv\n",
    "f = open('predictions_45features.csv', 'w')\n",
    "f.write('Business_ID,Is_Bankrupted\\n')\n",
    "for a,b in zip(cID, clf.predict(t1[XGBFeatures])):\n",
    "    f.write(str(a))\n",
    "    f.write(',')\n",
    "    f.write(str(round(b)))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE Re-Sampling:\n",
    "sm = SMOTE(random_state=12)\n",
    "x_train_sm, y_train_sm = sm.fit_sample(x_training[XGBFeatures], y_training)\n",
    "x_train_sm = pd.DataFrame(x_train_sm, columns=XGBFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9997342545841085 Test Score:0.9846311475409836\n",
      "Test F1:0.7272727272727272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[941,  10],\n",
       "       [  5,  20]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = XGBClassifier(max_depth=3, learning_rate=0.25, n_estimators=125, verbosity=1, objective='binary:logistic',\n",
    "                    booster='gbtree', n_jobs=-1, gamma=0, min_child_weight=0,\n",
    "                    max_delta_step=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0,\n",
    "                    missing=None)\n",
    "clf.fit(x_train_sm, y_train_sm)\n",
    "train_score = clf.score(x_train_sm, y_train_sm)\n",
    "test_score = clf.score(x_testing[XGBFeatures], y_testing)\n",
    "test_f1 = f1_score(y_testing, clf.predict(x_testing[XGBFeatures]))\n",
    "print('Train Score:{} Test Score:{}'.format(train_score, test_score))\n",
    "print('Test F1:{}'.format(test_f1))\n",
    "confusion_matrix(clf.predict(x_testing[XGBFeatures]),y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results to predictions.csv\n",
    "f = open('predictions_upsampled.csv', 'w')\n",
    "f.write('Business_ID,Is_Bankrupted\\n')\n",
    "for a,b in zip(cID, clf.predict(t1[XGBFeatures])):\n",
    "    f.write(str(a))\n",
    "    f.write(',')\n",
    "    f.write(str(round(b)))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        #self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        #self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(45, 100)\n",
    "        self.do1 = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(100, 150)\n",
    "        self.fc3 = nn.Linear(150, 200)\n",
    "        self.fc4 = nn.Linear(200, 100)\n",
    "        self.fc5 = nn.Linear(100, 50)\n",
    "        self.fc6 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        #x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.do1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.do1(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.sigmoid(self.fc6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=45, out_features=100, bias=True)\n",
      "  (do1): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=100, out_features=150, bias=True)\n",
      "  (fc3): Linear(in_features=150, out_features=200, bias=True)\n",
      "  (fc4): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc5): Linear(in_features=100, out_features=50, bias=True)\n",
      "  (fc6): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "weight = torch.tensor([0.1, 0.9])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4786) tensor([0.])\n",
      "[1,   200] loss: 0.06951\n",
      "tensor(0.4914) tensor([1.])\n",
      "[1,   400] loss: 0.06910\n",
      "tensor(0.5169) tensor([1.])\n",
      "[1,   600] loss: 0.06775\n",
      "tensor(0.2694) tensor([0.])\n",
      "[1,   800] loss: 0.06575\n",
      "tensor(0.5084) tensor([0.])\n",
      "[1,  1000] loss: 0.06171\n",
      "tensor(0.7161) tensor([1.])\n",
      "[2,   200] loss: 0.05820\n",
      "tensor(0.5658) tensor([1.])\n",
      "[2,   400] loss: 0.06317\n",
      "tensor(0.3311) tensor([0.])\n",
      "[2,   600] loss: 0.05965\n",
      "tensor(0.5221) tensor([1.])\n",
      "[2,   800] loss: 0.06109\n",
      "tensor(0.2724) tensor([1.])\n",
      "[2,  1000] loss: 0.05210\n",
      "tensor(0.0863) tensor([0.])\n",
      "[3,   200] loss: 0.05748\n",
      "tensor(0.2326) tensor([0.])\n",
      "[3,   400] loss: 0.05697\n",
      "tensor(0.6844) tensor([0.])\n",
      "[3,   600] loss: 0.05287\n",
      "tensor(0.6815) tensor([1.])\n",
      "[3,   800] loss: 0.06113\n",
      "tensor(0.6350) tensor([1.])\n",
      "[3,  1000] loss: 0.05217\n",
      "tensor(0.6672) tensor([1.])\n",
      "[4,   200] loss: 0.05607\n",
      "tensor(0.7163) tensor([1.])\n",
      "[4,   400] loss: 0.05061\n",
      "tensor(0.7142) tensor([1.])\n",
      "[4,   600] loss: 0.04904\n",
      "tensor(0.2661) tensor([1.])\n",
      "[4,   800] loss: 0.04861\n",
      "tensor(0.0811) tensor([0.])\n",
      "[4,  1000] loss: 0.04910\n",
      "tensor(0.6161) tensor([0.])\n",
      "[5,   200] loss: 0.05933\n",
      "tensor(0.3287) tensor([1.])\n",
      "[5,   400] loss: 0.04865\n",
      "tensor(0.7323) tensor([0.])\n",
      "[5,   600] loss: 0.05614\n",
      "tensor(0.0289) tensor([0.])\n",
      "[5,   800] loss: 0.05669\n",
      "tensor(0.1251) tensor([1.])\n",
      "[5,  1000] loss: 0.05271\n",
      "tensor(0.0762) tensor([0.])\n",
      "[6,   200] loss: 0.04255\n",
      "tensor(0.4112) tensor([1.])\n",
      "[6,   400] loss: 0.05431\n",
      "tensor(0.2734) tensor([0.])\n",
      "[6,   600] loss: 0.05311\n",
      "tensor(0.1724) tensor([0.])\n",
      "[6,   800] loss: 0.04887\n",
      "tensor(0.7924) tensor([1.])\n",
      "[6,  1000] loss: 0.04386\n",
      "tensor(0.7584) tensor([1.])\n",
      "[7,   200] loss: 0.04429\n",
      "tensor(0.7188) tensor([1.])\n",
      "[7,   400] loss: 0.04734\n",
      "tensor(0.1619) tensor([0.])\n",
      "[7,   600] loss: 0.04306\n",
      "tensor(0.6055) tensor([1.])\n",
      "[7,   800] loss: 0.04976\n",
      "tensor(0.0149) tensor([0.])\n",
      "[7,  1000] loss: 0.04508\n",
      "tensor(0.4447) tensor([1.])\n",
      "[8,   200] loss: 0.05084\n",
      "tensor(0.7604) tensor([0.])\n",
      "[8,   400] loss: 0.04110\n",
      "tensor(0.9262) tensor([1.])\n",
      "[8,   600] loss: 0.05315\n",
      "tensor(0.7254) tensor([0.])\n",
      "[8,   800] loss: 0.04600\n",
      "tensor(0.8126) tensor([1.])\n",
      "[8,  1000] loss: 0.03948\n",
      "tensor(0.9210) tensor([1.])\n",
      "[9,   200] loss: 0.04659\n",
      "tensor(0.2153) tensor([0.])\n",
      "[9,   400] loss: 0.04136\n",
      "tensor(0.7966) tensor([1.])\n",
      "[9,   600] loss: 0.05173\n",
      "tensor(0.0158) tensor([0.])\n",
      "[9,   800] loss: 0.04351\n",
      "tensor(0.7944) tensor([0.])\n",
      "[9,  1000] loss: 0.04982\n",
      "tensor(0.4454) tensor([1.])\n",
      "[10,   200] loss: 0.04202\n",
      "tensor(6.2172e-12) tensor([0.])\n",
      "[10,   400] loss: 0.04407\n",
      "tensor(0.8366) tensor([1.])\n",
      "[10,   600] loss: 0.04477\n",
      "tensor(0.5988) tensor([0.])\n",
      "[10,   800] loss: 0.04393\n",
      "tensor(0.0703) tensor([0.])\n",
      "[10,  1000] loss: 0.04105\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, 1000):\n",
    "        randn = np.random.randint(0, len(x_train_sm))\n",
    "        x_train = torch.tensor(x_train_sm.iloc[randn].values).float()\n",
    "        y_train = torch.tensor([y_train_sm[randn]]).float()\n",
    "        #weight_ = weight[y_train.data.view(-1).long()].view_as(y_train)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss_class_weighted = loss * weight_\n",
    "        loss_class_weighted = loss_class_weighted.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i %200 == 199:    # print every 200 mini-batches\n",
    "            print(outputs.data[0], y_train)\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[651,   9],\n",
       "       [295,  21]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.1213872832369942"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "correct_bankruptcy = 0\n",
    "incorrect_bankruptcy = 0\n",
    "test_output = []\n",
    "for i in range(0, len(x_testing.index)):\n",
    "    ref_x_test1 = torch.tensor(x_testing[XGBFeatures].iloc[i].values).float()\n",
    "    ref_y_test1 = torch.tensor(y_testing.iloc[i]).float()\n",
    "    outputs = net(ref_x_test1)\n",
    "    test_output.append(round(outputs.item()))\n",
    "display(confusion_matrix(test_output,y_testing))\n",
    "f1_score(y_testing,test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
